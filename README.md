# ML25-26-08-LLM-Prompt-Testing-Semantic-Assertions-SemanticEval
Testing framework for evaluating LLM prompt outputs using semantic similarity and LLM-based assertions.

## Project Overview
This project implements a testing framework for evaluating LLM (Large Language Model) prompts using **semantic assertions**.  
It verifies that the outputs from LLMs match the intended meaning of prompts, even if the wording varies.

## Team - SemanticEval
- **Muhammad Bilal - Matriculation number: 1631825**  
- **Amjad Aziz - Matriculation Number: 1629013**
